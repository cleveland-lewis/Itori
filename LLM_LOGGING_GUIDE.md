# LLM Feature Usage - Developer Console Logging Guide

## Overview
Comprehensive logging for all LLM/AI feature usage in the app when Developer Mode is enabled.

## How to Enable

**Settings ‚Üí Developer ‚Üí Enable Developer Mode + Data Logging**

All LLM logs use the category **"LLM"** for easy filtering.

## Log Events

### 1Ô∏è‚É£ LLM Request Started (Normal Flow)

```
LOG: ü§ñ Starting LLM request
  provider: OpenAI | Anthropic | LocalLLM | etc
  port: assignmentParser | questionGenerator | etc
  trigger: <UUID> (request ID)
  privacy: normal | redacted | anonymous
  inputSize: 1234 bytes
  timestamp: 2026-01-04 00:05:23
```

**What this means:**
- User triggered an AI feature
- LLM provider is about to be called
- Network request will be made (unless local)

### 2Ô∏è‚É£ LLM Request Completed Successfully

```
LOG: ‚úÖ LLM request completed
  provider: OpenAI
  port: assignmentParser
  duration: 2.347s
  outputSize: 892 bytes
  modelUsed: gpt-4
  tokensUsed: 245
  success: true
```

**What this means:**
- LLM call succeeded
- Shows actual model used (may differ from requested)
- Token usage for billing/quota tracking
- Response time for performance monitoring

### 3Ô∏è‚É£ Output Preview (Debug Level)

```
LOG: Output preview
  provider: OpenAI
  preview: {"assignments":[{"title":"Math Homework","dueDate":"2026-01-10","category":"homework"}],"confidence":0.95}...
```

**What this means:**
- First 200 characters of LLM output
- Useful for debugging parsing issues
- Only shown at DEBUG log level

### 4Ô∏è‚É£ LLM Request Failed

```
LOG: ‚ùå LLM request failed
  provider: OpenAI
  port: assignmentParser
  duration: 5.023s
  error: Network timeout
  willRetry: false
```

**What this means:**
- LLM call failed (network, API error, timeout)
- Provider marked as unreliable
- Will fallback to deterministic algorithm

### 5Ô∏è‚É£ LLM Disabled - Using Fallback

```
LOG: üö´ LLM assistance disabled - using fallback only
  port: assignmentParser
  trigger: <UUID>
  reason: user_setting_disabled
  
LOG: Fallback completed (LLM disabled)
  port: assignmentParser
  duration: 0.023s
```

**What this means:**
- User has LLM assistance turned OFF in settings
- Using deterministic algorithm instead
- No network calls made
- Much faster (no API latency)

### 6Ô∏è‚É£ Fallback Strategy (Realtime Ports)

```
LOG: üîÑ Using deterministic fallback (no LLM)
  port: realtimeParser
  reason: fallback-first strategy
  trigger: <UUID>

LOG: Fallback completed
  port: realtimeParser
  duration: 0.015s
  deterministic: true
```

**What this means:**
- Port is configured for fallback-first
- Prioritizes speed over AI enhancement
- Used for realtime features (autocomplete, etc)

### 7Ô∏è‚É£ No Fallback Available (Error)

```
LOG: ‚ùå No fallback available for port
  port: imageAnalysis
  supportsFallback: false
```

**What this means:**
- LLM is disabled but port requires AI
- Feature will not work without LLM
- User needs to enable LLM assistance

## Complete Flow Examples

### Example 1: Parse Syllabus with LLM Enabled

```
1. ü§ñ Starting LLM request
   provider: OpenAI
   port: syllabusParser
   inputSize: 15234 bytes (PDF text)
   
2. ‚úÖ LLM request completed
   duration: 3.452s
   outputSize: 2341 bytes
   modelUsed: gpt-4-turbo
   tokensUsed: 3421
   
3. Output preview
   preview: {"course":"MATH 101","assignments":[...],"schedule":[...]}
```

### Example 2: Parse Syllabus with LLM Disabled

```
1. üö´ LLM assistance disabled - using fallback only
   port: syllabusParser
   reason: user_setting_disabled
   
2. Fallback completed (LLM disabled)
   duration: 0.234s
   
Result: Deterministic parser used (regex-based)
```

### Example 3: LLM Request Fails, Falls Back

```
1. ÔøΩÔøΩ Starting LLM request
   provider: Anthropic
   port: questionGenerator
   
2. ‚ùå LLM request failed
   error: API rate limit exceeded
   duration: 0.523s
   
3. üîÑ Using deterministic fallback (no LLM)
   reason: provider failed
   
4. Fallback completed
   duration: 0.089s
```

## What Each Port Does

| Port ID | Feature | What it does |
|---------|---------|--------------|
| `assignmentParser` | Parse syllabus text | Extracts assignments from PDFs/text |
| `questionGenerator` | Generate test questions | Creates practice questions from content |
| `flashcardGenerator` | Generate flashcards | Creates study flashcards |
| `summaryGenerator` | Summarize text | Creates summaries of readings |
| `scheduleOptimizer` | Optimize study schedule | AI-enhanced scheduling |

## Privacy Levels

| Level | Description | What's sent to LLM |
|-------|-------------|-------------------|
| `normal` | Default | Full content with redaction policy |
| `redacted` | Sensitive content removed | PII stripped before sending |
| `anonymous` | Maximum privacy | Fully anonymized data only |

## Provider Types

| Provider | Description | Notes |
|----------|-------------|-------|
| `OpenAI` | ChatGPT/GPT-4 | Cloud-based, requires API key |
| `Anthropic` | Claude | Cloud-based, requires API key |
| `LocalLLM` | On-device | MLX models, no network |
| `Ollama` | Local server | Self-hosted, localhost |

## Performance Benchmarks

**Expected durations:**
- LLM requests: 1-5 seconds (network + inference)
- Fallback: 0.01-0.5 seconds (local computation)
- Local LLM: 2-10 seconds (on-device inference)

**If you see:**
- `duration > 10s` ‚Üí Check network/API status
- `duration > 30s` ‚Üí Timeout likely, check logs for error

## Debugging Scenarios

### Scenario 1: Feature Not Working

**Check logs for:**
```
üö´ LLM assistance disabled ‚Üí User needs to enable in Settings
‚ùå No fallback available ‚Üí Feature requires LLM
‚ùå LLM request failed ‚Üí Provider issue
```

### Scenario 2: Slow Performance

**Check logs for:**
```
duration: 15.234s ‚Üí API slow, consider timeout adjustment
provider: OpenAI, modelUsed: gpt-4 ‚Üí Try faster model
tokensUsed: 50000 ‚Üí Input too large, needs chunking
```

### Scenario 3: Unexpected Results

**Check logs for:**
```
Output preview ‚Üí See actual LLM output
privacy: redacted ‚Üí Some content was stripped
modelUsed: gpt-3.5-turbo ‚Üí Different from requested gpt-4
```

## Filtering Console Logs

**Xcode Console:**
```
Search: "LLM"
```

**Console.app:**
```
category:LLM
subsystem:Itori category:LLM
```

**Only LLM requests:**
```
"Starting LLM request"
```

**Only failures:**
```
"LLM request failed"
```

**Only completions:**
```
"LLM request completed"
```

## Token Usage Tracking

Every successful LLM request logs `tokensUsed` for:
- Cost estimation (tokens ‚Üí API costs)
- Quota monitoring (rate limits)
- Performance analysis (more tokens = slower)

**Typical token usage:**
- Syllabus parsing: 1000-5000 tokens
- Question generation: 500-2000 tokens
- Flashcard generation: 300-1500 tokens
- Summary generation: 200-1000 tokens

## Privacy & Security

**What's logged:**
- Provider name
- Port ID (feature type)
- Duration, token count, model used
- Request ID (for tracing)
- **NOT logged:** Actual user content, API keys

**Output preview:**
- Only first 200 chars
- Only at DEBUG level
- Can contain user data - careful when sharing logs

## Pro Tips

1. **Filter by port** to track specific features
2. **Watch duration** to identify performance issues
3. **Check tokensUsed** to optimize API costs
4. **Compare provider** performance
5. **Monitor fallback usage** to see reliability
6. **Track trigger IDs** across multiple operations

## Testing Checklist

- [ ] Enable LLM ‚Üí see "Starting LLM request"
- [ ] Disable LLM ‚Üí see "LLM assistance disabled"
- [ ] Successful request ‚Üí see duration + tokens
- [ ] Failed request ‚Üí see error message
- [ ] Fallback ‚Üí see "Using deterministic fallback"
- [ ] Output preview ‚Üí see JSON snippet
- [ ] Multiple providers ‚Üí see different provider names
- [ ] Privacy levels ‚Üí verify redaction applied

